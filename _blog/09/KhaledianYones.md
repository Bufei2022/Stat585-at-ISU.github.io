---
title: "A Series of Tubes..."
author: "Yones Khaledian"
topic: "09"
layout: post
root: ../../../
output: 
  html_document: 
    #css: extra.css
---

### Background

The internet may not be [a series of tubes](https://en.wikipedia.org/wiki/Series_of_tubes), but it is made up of a number of different programming and markup languages. It is useful to develop a basic vocabulary and understanding of these languages before attempting to interact with (messy) data stored on the internet. 

<div class="click-to-top">
<a href="https://www.xkcd.com/1144/"><img src="https://imgs.xkcd.com/comics/tags.png" alt="XKCD comic: HTML Tags" /></a>
<span>\<A\>: Like \</a\>this.\&nbsp;"</span>
</div>

As the reading material for this week, you are asked to read through a set of websites on XML and HTML and navigating through them:

- **XML (eXtensible Markup Language):**
XML is a generic markup framework. Many different common file types are based on or related to XML structure (for instance, .docx, .xlsx, .html) <bR>
Read the following sections of w3schools.com's Introduction to XML:

    - [XML Introduction](https://www.w3schools.com/xml/xml_whatis.asp)
    - [XML How to Use](https://www.w3schools.com/xml/xml_usedfor.asp)
    - [XML Tree](https://www.w3schools.com/xml/xml_tree.asp)
    - [XML Syntax](https://www.w3schools.com/xml/xml_syntax.asp)
    - [XML Elements](https://www.w3schools.com/xml/xml_elements.asp)
    - [XML Attributes](https://www.w3schools.com/xml/xml_attributes.asp)

- **HTML (HyperText Markup Language):** HTML focuses on the display of information in a document format. XML is a much more general framework, but most of the concepts (tags, attributes, elements) apply directly to HTML. Open up the [w3schools HTML page](https://www.w3schools.com/html/default.asp) and read the introduction, then look through a few topics in the tutorial that interest you. 

- **Navigation: HTML, CSS, XPATH, and more:** In many situations, it is helpful to be able to pick out specific parts of an HTML or XML file - for example, a table with useful data. CSS (Cascading Style Sheets) Selectors and XPATH are two methods commonly used to identify specific nodes in HTML or XML documents. 

    - [XPATH Syntax](https://www.w3schools.com/xml/xpath_syntax.asp)
    - [CSS Selector Syntax](https://www.w3schools.com/cssref/css_selectors.asp)

**Write a blog post answering the following questions and detailing the progress: **

1. The `xml2` R package can be used to work with xml files. Write a function, `current_weather` that accepts a 4-letter airport code (KAMW in the URL here: https://w1.weather.gov/xml/current_obs/KAMW.xml) and returns a data frame with the airport location (station ID, latitude, longitude), last update time, and current weather information (temperature, weather condition, wind speed and direction) at that airport. The `xml2` functions `read_xml`, `xml_children`, `xml_name`, and `xml_text` will be useful. Remember to handle errors and check inputs, and make sure to return a data frame with appropriate data types. 


```r
library(xml2)

# Read xml 
weather <- read_xml("https://w1.weather.gov/xml/current_obs/KAMW.xml")
# Return xml
xml_children(weather)
```

```
## {xml_nodeset (38)}
##  [1] <credit>NOAA's National Weather Service</credit>
##  [2] <credit_URL>http://weather.gov/</credit_URL>
##  [3] <image>\n  <url>http://weather.gov/images/xml_logo.gif</url>\n  <ti ...
##  [4] <suggested_pickup>15 minutes after the hour</suggested_pickup>
##  [5] <suggested_pickup_period>60</suggested_pickup_period>
##  [6] <location>Ames, Ames Municipal Airport, IA</location>
##  [7] <station_id>KAMW</station_id>
##  [8] <latitude>41.99056</latitude>
##  [9] <longitude>-93.61889</longitude>
## [10] <observation_time>Last Updated on Apr 3 2019, 12:53 pm CDT</observa ...
## [11] <observation_time_rfc822>Wed, 03 Apr 2019 12:53:00 -0500</observati ...
## [12] <weather>Overcast</weather>
## [13] <temperature_string>54.0 F (12.2 C)</temperature_string>
## [14] <temp_f>54.0</temp_f>
## [15] <temp_c>12.2</temp_c>
## [16] <relative_humidity>45</relative_humidity>
## [17] <wind_string>South at 9.2 MPH (8 KT)</wind_string>
## [18] <wind_dir>South</wind_dir>
## [19] <wind_degrees>180</wind_degrees>
## [20] <wind_mph>9.2</wind_mph>
## ...
```

```r
# The name of an xml element.
xml_name(weather)
```

```
## [1] "current_observation"
```

```r
# xml_text returns a character vector.
xml_text(weather)
```

```
## [1] "NOAA's National Weather Servicehttp://weather.gov/http://weather.gov/images/xml_logo.gifNOAA's National Weather Servicehttp://weather.gov15 minutes after the hour60Ames, Ames Municipal Airport, IAKAMW41.99056-93.61889Last Updated on Apr 3 2019, 12:53 pm CDTWed, 03 Apr 2019 12:53:00 -0500Overcast54.0 F (12.2 C)54.012.245South at 9.2 MPH (8 KT)South1809.281023.4 mb1023.430.2233.1 F (0.6 C)33.10.651 F (11 C)511110.00http://forecast.weather.gov/images/wtf/small/http://www.weather.gov/data/obhistory/KAMW.htmlovc.pnghttp://www.weather.gov/data/METAR/KAMW.1.txthttp://weather.gov/disclaimer.htmlhttp://weather.gov/disclaimer.htmlhttp://weather.gov/notice.html"
```

```r
xml_text(xml_children(weather))
```

```
##  [1] "NOAA's National Weather Service"                                                        
##  [2] "http://weather.gov/"                                                                    
##  [3] "http://weather.gov/images/xml_logo.gifNOAA's National Weather Servicehttp://weather.gov"
##  [4] "15 minutes after the hour"                                                              
##  [5] "60"                                                                                     
##  [6] "Ames, Ames Municipal Airport, IA"                                                       
##  [7] "KAMW"                                                                                   
##  [8] "41.99056"                                                                               
##  [9] "-93.61889"                                                                              
## [10] "Last Updated on Apr 3 2019, 12:53 pm CDT"                                               
## [11] "Wed, 03 Apr 2019 12:53:00 -0500"                                                        
## [12] "Overcast"                                                                               
## [13] "54.0 F (12.2 C)"                                                                        
## [14] "54.0"                                                                                   
## [15] "12.2"                                                                                   
## [16] "45"                                                                                     
## [17] "South at 9.2 MPH (8 KT)"                                                                
## [18] "South"                                                                                  
## [19] "180"                                                                                    
## [20] "9.2"                                                                                    
## [21] "8"                                                                                      
## [22] "1023.4 mb"                                                                              
## [23] "1023.4"                                                                                 
## [24] "30.22"                                                                                  
## [25] "33.1 F (0.6 C)"                                                                         
## [26] "33.1"                                                                                   
## [27] "0.6"                                                                                    
## [28] "51 F (11 C)"                                                                            
## [29] "51"                                                                                     
## [30] "11"                                                                                     
## [31] "10.00"                                                                                  
## [32] "http://forecast.weather.gov/images/wtf/small/"                                          
## [33] "http://www.weather.gov/data/obhistory/KAMW.html"                                        
## [34] "ovc.png"                                                                                
## [35] "http://www.weather.gov/data/METAR/KAMW.1.txt"                                           
## [36] "http://weather.gov/disclaimer.html"                                                     
## [37] "http://weather.gov/disclaimer.html"                                                     
## [38] "http://weather.gov/notice.html"
```


2. Which HTML tags did you investigate? Describe how to format at least 3 separate pieces of a document using HTML tags.

  <head> contains information about the document. 
  <title> shows a specific title for the document.
  <html> displays the root element of an HTML page.

3. Compile this Rmarkdown document to HTML, then open the HTML file in a web browser. Open the inspector console for your browser (Ctrl-Shift-I in Chrome, Ctrl-Shift-C in Firefox) and look at the HTML code corresponding to various parts of the document. <br>
Answer the following questions:

    - What types of tags did you find? 
  I found <head>, </html>, <body>, </style>, </div>
    
    - How are code chunks formatted in HTML?
  There are two major groups: </head>, </body>. And each of them has some subgroups.  

    - What differences are there in the HTML markup for R code chunks and R output blocks?
  I think that R output blocks are more compact than R code chuncks. In block, users can write very specific codes. 
    
    
4. In R, the `rvest` package, which is part of the tidyverse, makes it (relatively) easy to pull specific pieces from structured documents. The `html_nodes` function selects nodes using either xpath or css, and additional functions such as `html_attrs`, `html_text`, and `html_table` pull information out of the markup text.<br>
Choose a Wikipedia page that has at least one image to test the `rvest` package out

Remember, just because you have the HTML file doesn't mean you should commit it to your git repository!!! Delete the HTML file now if you're going to be tempted to accidentally commit and push it.


```r
library(rvest)

MLR_html <- read_html("https://en.wikipedia.org/wiki/Linear_regression")

MLR_html %>% html_nodes("center") %>% html_nodes("font") # Select nodes
```

```
## {xml_nodeset (0)}
```

```r
html_text(MLR_html) # Extract text
```

```
## [1] "Linear regression - Wikipediadocument.documentElement.className = document.documentElement.className.replace( /(^|\\s)client-nojs(\\s|$)/, \"$1client-js$2\" );(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Linear_regression\",\"wgTitle\":\"Linear regression\",\"wgCurRevisionId\":889485632,\"wgRevisionId\":889485632,\"wgArticleId\":48758386,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"Articles with inconsistent citation formats\",\"Webarchive template wayback links\",\"Wikipedia articles needing clarification from May 2018\",\"Wikipedia articles needing clarification from March 2012\",\"All articles with unsourced statements\",\"Articles with unsourced statements from June 2018\",\"Articles to be expanded from January 2010\",\"All articles to be expanded\",\"Articles using small message boxes\",\"Single-equation methods (econometrics)\",\"Estimation theory\",\"Parametric statistics\"],\"wgBreakFrames\":false,\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgMonthNamesShort\":[\"\",\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"],\"wgRelevantPageName\":\"Linear_regression\",\"wgRelevantArticleId\":48758386,\"wgRequestId\":\"XJwzIgpAICkAAJuUlFkAAABG\",\"wgCSPNonce\":false,\"wgIsProbablyEditable\":true,\"wgRelevantPageIsProbablyEditable\":true,\"wgRestrictionEdit\":[],\"wgRestrictionMove\":[],\"wgFlaggedRevsParams\":{\"tags\":{}},\"wgStableRevisionId\":null,\"wgCategoryTreePageCategoryOptions\":\"{\\\"mode\\\":0,\\\"hideprefix\\\":20,\\\"showcount\\\":true,\\\"namespaces\\\":false}\",\"wgWikiEditorEnabledModules\":[],\"wgBetaFeaturesFeatures\":[],\"wgMediaViewerOnClick\":true,\"wgMediaViewerEnabledByDefault\":true,\"wgPopupsReferencePreviews\":false,\"wgPopupsShouldSendModuleToUser\":true,\"wgPopupsConflictsWithNavPopupGadget\":false,\"wgVisualEditor\":{\"pageLanguageCode\":\"en\",\"pageLanguageDir\":\"ltr\",\"pageVariantFallbacks\":\"en\",\"usePageImages\":true,\"usePageDescriptions\":true},\"wgMFDisplayWikibaseDescriptions\":{\"search\":true,\"nearby\":true,\"watchlist\":true,\"tagline\":false},\"wgRelatedArticles\":null,\"wgRelatedArticlesUseCirrusSearch\":true,\"wgRelatedArticlesOnlyUseCirrusSearch\":false,\"wgWMESchemaEditAttemptStepOversample\":false,\"wgPoweredByHHVM\":true,\"wgULSCurrentAutonym\":\"English\",\"wgNoticeProject\":\"wikipedia\",\"wgCentralNoticeCookiesToDelete\":[],\"wgCentralNoticeCategoriesUsingLegacy\":[\"Fundraising\",\"fundraising\"],\"wgWikibaseItemId\":\"Q10861030\",\"wgScoreNoteLanguages\":{\"arabic\":\"العربية\",\"catalan\":\"català\",\"deutsch\":\"Deutsch\",\"english\":\"English\",\"espanol\":\"español\",\"italiano\":\"italiano\",\"nederlands\":\"Nederlands\",\"norsk\":\"norsk\",\"portugues\":\"português\",\"suomi\":\"suomi\",\"svenska\":\"svenska\",\"vlaams\":\"West-Vlams\"},\"wgScoreDefaultNoteLanguage\":\"nederlands\",\"wgCentralAuthMobileDomain\":false,\"wgCodeMirrorEnabled\":true,\"wgVisualEditorToolbarScrollOffset\":0,\"wgVisualEditorUnsupportedEditParams\":[\"undo\",\"undoafter\",\"veswitched\"],\"wgEditSubmitButtonLabelPublish\":true,\"oresWikiId\":\"enwiki\",\"oresBaseUrl\":\"http://ores.discovery.wmnet:8081/\",\"oresApiVersion\":3});mw.loader.state({\"ext.gadget.charinsert-styles\":\"ready\",\"ext.globalCssJs.user.styles\":\"ready\",\"ext.globalCssJs.site.styles\":\"ready\",\"site.styles\":\"ready\",\"noscript\":\"ready\",\"user.styles\":\"ready\",\"ext.globalCssJs.user\":\"ready\",\"ext.globalCssJs.site\":\"ready\",\"user\":\"ready\",\"user.options\":\"ready\",\"user.tokens\":\"loading\",\"ext.cite.styles\":\"ready\",\"ext.math.styles\":\"ready\",\"mediawiki.legacy.shared\":\"ready\",\"mediawiki.legacy.commonPrint\":\"ready\",\"mediawiki.toc.styles\":\"ready\",\"wikibase.client.init\":\"ready\",\"ext.visualEditor.desktopArticleTarget.noscript\":\"ready\",\"ext.uls.interlanguage\":\"ready\",\"ext.wikimediaBadges\":\"ready\",\"ext.3d.styles\":\"ready\",\"mediawiki.skinning.interface\":\"ready\",\"skins.vector.styles\":\"ready\"});mw.loader.implement(\"user.tokens@0tffind\",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({\"editToken\":\"+\\\\\",\"patrolToken\":\"+\\\\\",\"watchToken\":\"+\\\\\",\"csrfToken\":\"+\\\\\"});\n});RLPAGEMODULES=[\"ext.cite.ux-enhancements\",\"ext.math.scripts\",\"ext.scribunto.logs\",\"site\",\"mediawiki.page.startup\",\"mediawiki.page.ready\",\"mediawiki.toc\",\"mediawiki.searchSuggest\",\"ext.gadget.teahouse\",\"ext.gadget.ReferenceTooltips\",\"ext.gadget.watchlist-notice\",\"ext.gadget.DRN-wizard\",\"ext.gadget.charinsert\",\"ext.gadget.refToolbar\",\"ext.gadget.extra-toolbar-buttons\",\"ext.gadget.switcher\",\"ext.centralauth.centralautologin\",\"mmv.head\",\"mmv.bootstrap.autostart\",\"ext.popups\",\"ext.visualEditor.desktopArticleTarget.init\",\"ext.visualEditor.targetLoader\",\"ext.eventLogging\",\"ext.wikimediaEvents\",\"ext.navigationTiming\",\"ext.uls.eventlogger\",\"ext.uls.init\",\"ext.uls.compactlinks\",\"ext.uls.interface\",\"ext.quicksurveys.init\",\"ext.centralNotice.geoIP\",\"ext.centralNotice.startUp\",\"skins.vector.js\"];mw.loader.load(RLPAGEMODULES);});\n\n\n\n\t\n\t\n\t\n\n\n\tLinear regression\n\t\n\t\n\t\tFrom Wikipedia, the free encyclopedia\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\tJump to navigation\n\t\tJump to search\n\t\tPart of a series on StatisticsRegression analysis\nModels\nLinear regression\nSimple regression\nPolynomial regression\nGeneral linear model\n\nGeneralized linear model\nDiscrete choice\nLogistic regression\nMultinomial logit\nMixed logit\nProbit\nMultinomial probit\nOrdered logit\nOrdered probit\nPoisson\n\nMultilevel model\nFixed effects\nRandom effects\nMixed model\n\nNonlinear regression\nNonparametric\nSemiparametric\nRobust\nQuantile\nIsotonic\nPrincipal components\nLeast angle\nLocal\nSegmented\n\nErrors-in-variables\n\nEstimation\nLeast squares\nLinear\nNon-linear\n\nOrdinary\nWeighted\nGeneralized\n\nPartial\nTotal\nNon-negative\nRidge regression\nRegularized\n\nLeast absolute deviations\nIteratively reweighted\nBayesian\nBayesian multivariate\n\nBackground\nRegression validation\nMean and predicted response\nErrors and residuals\nGoodness of fit\nStudentized residual\nGauss–Markov theorem\n\n Statistics portalvteIn statistics, linear regression is a linear approach to modelling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression.[1] This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.[2]In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models.[3] Most commonly, the conditional mean of the response given the values of the explanatory variables (or predictors) is assumed to be an affine function of those values; less commonly, the conditional median or some other quantile is used. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis.\nLinear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications.[4] This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.\nLinear regression has many practical uses. Most applications fall into one of the following two broad categories:\n\nIf the goal is prediction, or forecasting, or error reduction,[clarification needed] linear regression can be used to fit a predictive model to an observed data set of values of the response and explanatory variables. After developing such a model, if additional values of the explanatory variables are collected without an accompanying response value, the fitted model can be used to make a prediction of the response.\nIf the goal is to explain variation in the response variable that can be attributed to variation in the explanatory variables, linear regression analysis can be applied to quantify the strength of the relationship between the response and the explanatory variables, and in particular to determine whether some explanatory variables may have no linear relationship with the response at all, or to identify which subsets of explanatory variables may contain redundant information about the response.Linear regression models are often fitted using the least squares approach, but they may also be fitted in other ways, such as by minimizing the \"lack of fit\" in some other norm (as with least absolute deviations regression), or by minimizing a penalized version of the least squares cost function as in ridge regression (L2-norm penalty) and lasso (L1-norm penalty). Conversely, the least squares approach can be used to fit models that are not linear models. Thus, although the terms \"least squares\" and \"linear model\" are closely linked, they are not synonymous.\n\nContents\n1 Introduction\n1.1 Assumptions\n1.2 Interpretation\n\n2 Extensions\n2.1 Simple and multiple linear regression\n2.2 General linear models\n2.3 Heteroscedastic models\n2.4 Generalized linear models\n2.5 Hierarchical linear models\n2.6 Errors-in-variables\n2.7 Others\n\n3 Estimation methods\n3.1 Least-squares estimation and related techniques\n3.2 Maximum-likelihood estimation and related techniques\n3.3 Other estimation techniques\n\n4 Applications\n4.1 Trend line\n4.2 Epidemiology\n4.3 Finance\n4.4 Economics\n4.5 Environmental science\n4.6 Machine learning\n\n5 History\n6 See also\n7 References\n7.1 Citations\n7.2 Sources\n\n8 Further reading\n9 External links\n\n\nIntroduction[edit]\n  In linear regression, the observations (red) are assumed to be the result of random deviations (green) from an underlying relationship (blue) between a dependent variable (y) and an independent variable (x).\nGiven a data set {yi,xi1,…,xip}i=1n{\\displaystyle \\{y_{i},\\,x_{i1},\\ldots ,x_{ip}\\}_{i=1}^{n}} of n statistical units, a linear regression model assumes that the relationship between the dependent variable y and the p-vector of regressors x is linear. This relationship is modeled through a disturbance term or error variable ε — an unobserved random variable that adds \"noise\" to the linear relationship between the dependent variable and regressors. Thus the model takes the form\n\nyi=β01+β1xi1+⋯+βpxip+εi=xiTβ+εi,i=1,…,n,{\\displaystyle y_{i}=\\beta _{0}1+\\beta _{1}x_{i1}+\\cdots +\\beta _{p}x_{ip}+\\varepsilon _{i}=\\mathbf {x} _{i}^{\\mathsf {T}}{\\boldsymbol {\\beta }}+\\varepsilon _{i},\\qquad i=1,\\ldots ,n,}where T denotes the transpose, so that xiTβ is the inner product between vectors xi and β.\nOften these n equations are stacked together and written in matrix notation as\n\ny=Xβ+ε,{\\displaystyle \\mathbf {y} =X{\\boldsymbol {\\beta }}+{\\boldsymbol {\\varepsilon }},\\,}where\n\ny=(y1y2⋮yn),{\\displaystyle \\mathbf {y} ={\\begin{pmatrix}y_{1}\\\\y_{2}\\\\\\vdots \\\\y_{n}\\end{pmatrix}},\\quad }X=(x1Tx2T⋮xnT)=(1x11⋯x1p1x21⋯x2p⋮⋮⋱⋮1xn1⋯xnp),{\\displaystyle X={\\begin{pmatrix}\\mathbf {x} _{1}^{\\mathsf {T}}\\\\\\mathbf {x} _{2}^{\\mathsf {T}}\\\\\\vdots \\\\\\mathbf {x} _{n}^{\\mathsf {T}}\\end{pmatrix}}={\\begin{pmatrix}1&x_{11}&\\cdots &x_{1p}\\\\1&x_{21}&\\cdots &x_{2p}\\\\\\vdots &\\vdots &\\ddots &\\vdots \\\\1&x_{n1}&\\cdots &x_{np}\\end{pmatrix}},}β=(β0β1β2⋮βp),ε=(ε1ε2⋮εn).{\\displaystyle {\\boldsymbol {\\beta }}={\\begin{pmatrix}\\beta _{0}\\\\\\beta _{1}\\\\\\beta _{2}\\\\\\vdots \\\\\\beta _{p}\\end{pmatrix}},\\quad {\\boldsymbol {\\varepsilon }}={\\begin{pmatrix}\\varepsilon _{1}\\\\\\varepsilon _{2}\\\\\\vdots \\\\\\varepsilon _{n}\\end{pmatrix}}.}Some remarks on notation and terminology:\n\ny{\\displaystyle \\mathbf {y} } is a vector of observed values yi (i=1,…,n){\\displaystyle y_{i}\\ (i=1,\\ldots ,n)} of the variable called the regressand, endogenous variable, response variable, measured variable, criterion variable, or dependent variable. This variable is also sometimes known as the predicted variable, but this should not be confused with predicted values, which are denoted y^{\\displaystyle {\\hat {y}}}. The decision as to which variable in a data set is modeled as the dependent variable and which are modeled as the independent variables may be based on a presumption that the value of one of the variables is caused by, or directly influenced by the other variables. Alternatively, there may be an operational reason to model one of the variables in terms of the others, in which case there need be no presumption of causality.\nX{\\displaystyle X} may be seen as a matrix of row-vectors xi{\\displaystyle \\mathbf {x} _{i}} or of n-dimensional column-vectors Xj{\\displaystyle X_{j}}, which are known as regressors, exogenous variables, explanatory variables, covariates, input variables, predictor variables, or independent variables (not to be confused with the concept of independent random variables). The matrix X{\\displaystyle X} is sometimes called the design matrix.\nUsually a constant is included as one of the regressors. In particular, xi0=1{\\displaystyle \\mathbf {x} _{i0}=1} for i=1,…,n{\\displaystyle i=1,\\ldots ,n}. The corresponding element of β is called the intercept. Many statistical inference procedures for linear models require an intercept to be present, so it is often included even if theoretical considerations suggest that its value should be zero.\nSometimes one of the regressors can be a non-linear function of another regressor or of the data, as in polynomial regression and segmented regression. The model remains linear as long as it is linear in the parameter vector β.\nThe values xij may be viewed as either observed values of random variables Xj or as fixed values chosen prior to observing the dependent variable. Both interpretations may be appropriate in different cases, and they generally lead to the same estimation procedures; however different approaches to asymptotic analysis are used in these two situations.\nβ{\\displaystyle {\\boldsymbol {\\beta }}} is a (p+1){\\displaystyle (p+1)}-dimensional parameter vector, where β0{\\displaystyle \\beta _{0}} is the intercept term (if one is included in the model—otherwise β{\\displaystyle {\\boldsymbol {\\beta }}} is p-dimensional). Its elements are known as effects or regression coefficients (although the latter term is sometimes reserved for the estimated effects). Statistical estimation and inference in linear regression focuses on β. The elements of this parameter vector are interpreted as the partial derivatives of the dependent variable with respect to the various independent variables.\nε{\\displaystyle {\\boldsymbol {\\varepsilon }}} is a vector of values εi{\\displaystyle \\varepsilon _{i}}. This part of the model is called the error term, disturbance term, or sometimes noise (in contrast with the \"signal\" provided by the rest of the model). This variable captures all other factors which influence the dependent variable y other than the regressors x. The relationship between the error term and the regressors, for example their correlation, is a crucial consideration in formulating a linear regression model, as it will determine the appropriate estimation method.Example. Consider a situation where a small ball is being tossed up in the air and then we measure its heights of ascent hi at various moments in time ti. Physics tells us that, ignoring the drag, the relationship can be modeled as\n\nhi=β1ti+β2ti2+εi,{\\displaystyle h_{i}=\\beta _{1}t_{i}+\\beta _{2}t_{i}^{2}+\\varepsilon _{i},}where β1 determines the initial velocity of the ball, β2 is proportional to the standard gravity, and εi is due to measurement errors. Linear regression can be used to estimate the values of β1 and β2 from the measured data. This model is non-linear in the time variable, but it is linear in the parameters β1 and β2; if we take regressors xi = (xi1, xi2)  = (ti, ti2), the model takes on the standard form\n\nhi=xiTβ+εi.{\\displaystyle h_{i}=\\mathbf {x} _{i}^{\\mathsf {T}}{\\boldsymbol {\\beta }}+\\varepsilon _{i}.}Assumptions[edit]\nSee also: Ordinary least squares § Assumptions\nStandard linear regression models with standard estimation techniques make a number of assumptions about the predictor variables, the response variables and their relationship.  Numerous extensions have been developed that allow each of these assumptions to be relaxed (i.e. reduced to a weaker form), and in some cases eliminated entirely. Generally these extensions make the estimation procedure more complex and time-consuming, and may also require more data in order to produce an equally precise model.\n\n  Example of a cubic polynomial regression, which is a type of linear regression.\nThe following are the major assumptions made by standard linear regression models with standard estimation techniques (e.g. ordinary least squares):\n\nWeak exogeneity.  This essentially means that the predictor variables x can be treated as fixed values, rather than random variables.  This means, for example, that the predictor variables are assumed to be error-free—that is, not contaminated with measurement errors. Although this assumption is not realistic in many settings, dropping it leads to significantly more difficult errors-in-variables models.\nLinearity.  This means that the mean of the response variable is a linear combination of the parameters (regression coefficients) and the predictor variables.  Note that this assumption is much less restrictive than it may at first seem.  Because the predictor variables are treated as fixed values (see above), linearity is really only a restriction on the parameters.  The predictor variables themselves can be arbitrarily transformed, and in fact multiple copies of the same underlying predictor variable can be added, each one transformed differently.  This technique is used, for example, in polynomial regression, which uses linear regression to fit the response variable as an arbitrary polynomial function (up to a given rank) of a predictor variable. This makes linear regression an extremely powerful inference method.  In fact, models such as polynomial regression are often \"too powerful\", in that they tend to overfit the data.  As a result, some kind of regularization must typically be used to prevent unreasonable solutions coming out of the estimation process.  Common examples are ridge regression and lasso regression.  Bayesian linear regression can also be used, which by its nature is more or less immune to the problem of overfitting. (In fact, ridge regression and lasso regression can both be viewed as    special cases of Bayesian linear regression, with particular types of prior distributions placed on the regression coefficients.)\nConstant variance (a.k.a. homoscedasticity).  This means that different values of the response variable have the same variance in their errors, regardless of the values of the predictor variables. In practice this assumption is invalid (i.e. the errors are heteroscedastic) if the response variable can vary over a wide scale. In order to check for heterogeneous error variance, or when a pattern of residuals violates model assumptions of homoscedasticity (error is equally variable around the 'best-fitting line' for all points of x), it is prudent to look for a \"fanning effect\" between residual error and predicted values. This is to say there will be a systematic change in the absolute or squared residuals when plotted against the predictive variables. Errors will not be evenly distributed across the regression line. Heteroscedasticity will result in the averaging over of distinguishable variances around the points to get a single variance that is inaccurately representing all the variances of the line. In effect, residuals appear clustered and spread apart on their predicted plots for larger and smaller values for points along the linear regression line, and the mean squared error for the model will be wrong. Typically, for example, a response variable whose mean is large will have a greater variance than one whose mean is small. For example, a given person whose income is predicted to be $100,000 may easily have an actual income of $80,000 or $120,000 (a standard deviation of around $20,000), while another person with a predicted income of $10,000 is unlikely to have the same $20,000 standard deviation, which would imply their actual income would vary anywhere between -$10,000 and $30,000. (In fact, as this shows, in many cases—often the same cases where the assumption of normally distributed errors fails—the variance or standard deviation should be predicted to be proportional to the mean, rather than constant.) Simple linear regression estimation methods give less precise parameter estimates and misleading inferential quantities such as standard errors when substantial heteroscedasticity is present. However, various estimation techniques (e.g. weighted least squares and heteroscedasticity-consistent standard errors) can handle heteroscedasticity in a quite general way. Bayesian linear regression techniques can also be used when the variance is assumed to be a function of the mean. It is also possible in some cases to fix the problem by applying a transformation to the response variable (e.g. fit the logarithm of the response variable using a linear regression model, which implies that the response variable has a log-normal distribution rather than a normal distribution).\nIndependence of errors.  This assumes that the errors of the response variables are uncorrelated with each other. (Actual statistical independence is a stronger condition than mere lack of correlation and is often not needed, although it can be exploited if it is known to hold.) Some methods (e.g. generalized least squares) are capable of handling correlated errors, although they typically require significantly more data unless some sort of regularization is used to bias the model towards assuming uncorrelated errors. Bayesian linear regression is a general way of handling this issue.\nLack of perfect multicollinearity in the predictors.  For standard least squares estimation methods, the design matrix X must have full column rank p; otherwise, we have a condition known as perfect multicollinearity in the predictor variables.  This can be triggered by having two or more perfectly correlated predictor variables (e.g. if the same predictor variable is mistakenly given twice, either without transforming one of the copies or by transforming one of the copies linearly). It can also happen if there is too little data available compared to the number of parameters to be estimated (e.g. fewer data points than regression coefficients). In the case of perfect multicollinearity, the parameter vector β will be non-identifiable—it has no unique solution.  At most we will be able to identify some of the parameters, i.e. narrow down its value to some linear subspace of Rp. See partial least squares regression.  Methods for fitting linear models with multicollinearity have been developed;[5][6][7][8] some require additional assumptions such as \"effect sparsity\"—that a large fraction of the effects are exactly zero. Note that the more computationally expensive iterated algorithms for parameter estimation, such as those used in generalized linear models, do not suffer from this problem.Beyond these assumptions, several other statistical properties of the data strongly influence the performance of different estimation methods:\n\nThe statistical relationship between the error terms and the regressors plays an important role in determining whether an estimation procedure has desirable sampling properties such as being unbiased and consistent.\nThe arrangement, or probability distribution of the predictor variables x has a major influence on the precision of estimates of β. Sampling and design of experiments are highly developed subfields of statistics that provide guidance for collecting data in such a way to achieve a precise estimate of β.Interpretation[edit]\n  The data sets in the Anscombe's quartet are designed to have approximately the same linear regression line (as well as nearly identical means, standard deviations, and correlations) but are graphically very different. This illustrates the pitfalls of relying solely on a fitted model to understand the relationship between variables.\nA fitted linear regression model can be used to identify the relationship between a single predictor variable xj and the response variable y when all the other predictor variables in the model are \"held fixed\". Specifically, the interpretation of βj is the expected change in y for a one-unit change in xj when the other covariates are held fixed—that is, the expected value of the partial derivative of y with respect to xj. This is sometimes called the unique effect of xj on y. In contrast, the marginal effect of xj on y can be assessed using a correlation coefficient or simple linear regression model relating only xj to y; this effect is the total derivative of y with respect to xj.\nCare must be taken when interpreting regression results, as some of the regressors may not allow for marginal changes (such as dummy variables, or the intercept term), while others cannot be held fixed (recall the example from the introduction: it would be impossible to \"hold ti fixed\" and at the same time change the value of ti2).\nIt is possible that the unique effect can be nearly zero even when the marginal effect is large. This may imply that some other covariate captures all the information in xj, so that once that variable is in the model, there is no contribution of xj to the variation in y. Conversely, the unique effect of xj can be large while its marginal effect is nearly zero. This would happen if the other covariates explained a great deal of the variation of y, but they mainly explain variation in a way that is complementary to what is captured by xj. In this case, including the other variables in the model reduces the part of the variability of y that is unrelated to xj, thereby strengthening the apparent relationship with xj.\nThe meaning of the expression \"held fixed\" may depend on how the values of the predictor variables arise. If the experimenter directly sets the values of the predictor variables according to a study design, the comparisons of interest may literally correspond to comparisons among units whose predictor variables have been \"held fixed\" by the experimenter. Alternatively, the expression \"held fixed\" can refer to a selection that takes place in the context of data analysis. In this case, we \"hold a variable fixed\" by restricting our attention to the subsets of the data that happen to have a common value for the given predictor variable. This is the only interpretation of \"held fixed\" that can be used in an observational study.\nThe notion of a \"unique effect\" is appealing when studying a complex system where multiple interrelated components influence the response variable. In some cases, it can literally be interpreted as the causal effect of an intervention that is linked to the value of a predictor variable. However, it has been argued that in many cases multiple regression analysis fails to clarify the relationships between the predictor variables and the response variable when the predictors are correlated with each other and are not assigned following a study design.[9]Commonality analysis may be helpful in disentangling the shared and unique impacts of correlated independent variables.[10]\nExtensions[edit]\nNumerous extensions of linear regression have been developed, which allow some or all of the assumptions underlying the basic model to be relaxed.\n\nSimple and multiple linear regression[edit]\n  Example of simple linear regression, which has one independent variable\nThe very simplest case of a single scalar predictor variable x and a single scalar response variable y is known as simple linear regression.  The extension to multiple and/or vector-valued predictor variables (denoted with a capital X) is known as multiple linear regression, also known as multivariable linear regression.  Nearly all real-world regression models involve multiple predictors, and basic descriptions of linear regression are often phrased in terms of the multiple regression model.  Note, however, that in these cases the response variable y is still a scalar. Another term, multivariate linear regression, refers to cases where y is a vector, i.e., the same as general linear regression.\n\nGeneral linear models[edit]\nThe general linear model considers the situation when the response variable is not a scalar (for each observation) but a vector, yi. Conditional linearity of E(yi|xi)=xiTB{\\displaystyle E(\\mathbf {y} _{i}|\\mathbf {x} _{i})=\\mathbf {x} _{i}^{\\mathsf {T}}B} is still assumed, with a matrix B replacing the vector β of the classical linear regression model. Multivariate analogues of Ordinary Least Squares (OLS) and Generalized Least Squares (GLS) have been developed. \"General linear models\" are also called \"multivariate linear models\". These are not the same as multivariable linear models (also called \"multiple linear models\").\n\nHeteroscedastic models[edit]\nVarious models have been created that allow for heteroscedasticity, i.e. the errors for different response variables may have different variances.  For example, weighted least squares is a method for estimating linear regression models when the response variables may have different error variances, possibly with correlated errors. (See also Weighted linear least squares, and Generalized least squares.) Heteroscedasticity-consistent standard errors is an improved method for use with uncorrelated but potentially heteroscedastic errors.\n\nGeneralized linear models[edit]\nGeneralized linear models (GLMs) are a framework for modeling response variables that are bounded or discrete. This is used, for example:\n\nwhen modeling positive quantities (e.g. prices or populations) that vary over a large scale—which are better described using a skewed distribution such as the log-normal distribution or Poisson distribution (although GLMs are not used for log-normal data, instead the response variable is simply transformed using the logarithm function);\nwhen modeling categorical data, such as the choice of a given candidate in an election (which is better described using a Bernoulli distribution/binomial distribution for binary choices, or a categorical distribution/multinomial distribution for multi-way choices), where there are a fixed number of choices that cannot be meaningfully ordered;\nwhen modeling ordinal data, e.g. ratings on a scale from 0 to 5, where the different outcomes can be ordered but where the quantity itself may not have any absolute meaning (e.g. a rating of 4 may not be \"twice as good\" in any objective sense as a rating of 2, but simply indicates that it is better than 2 or 3 but not as good as 5).Generalized linear models allow for an arbitrary link function, g, that relates the mean of the response variable(s) to the predictors: E(Y)=g−1(XB){\\displaystyle E(Y)=g^{-1}(XB)}. The link function is often related to the distribution of the response, and in particular it typically has the effect of transforming between the (−∞,∞){\\displaystyle (-\\infty ,\\infty )} range of the linear predictor and the range of the response variable.\nSome common examples of GLMs are:\n\nPoisson regression for count data.\nLogistic regression and probit regression for binary data.\nMultinomial logistic regression and multinomial probit regression for categorical data.\nOrdered logit and ordered probit regression for ordinal data.Single index models[clarification needed] allow some degree of nonlinearity in the relationship between x and y, while preserving the central role of the linear predictor β′x as in the classical linear regression model. Under certain conditions, simply applying OLS to data from a single-index model will consistently estimate β up to a proportionality constant.[11]\nHierarchical linear models[edit]\nHierarchical linear models (or multilevel regression) organizes the data into a hierarchy of regressions, for example where A is regressed on B, and B is regressed on C. It is often used where the variables of interest have a natural hierarchical structure such as in educational statistics, where students are nested in classrooms, classrooms are nested in schools, and schools are nested in some administrative grouping, such as a school district. The response variable might be a measure of student achievement such as a test score, and different covariates would be collected at the classroom, school, and school district levels.\n\nErrors-in-variables[edit]\nErrors-in-variables models (or \"measurement error models\") extend the traditional linear regression model to allow the predictor variables X to be observed with error. This error causes standard estimators of β to become biased. Generally, the form of bias is an attenuation, meaning that the effects are biased toward zero.\n\nOthers[edit]\nIn Dempster–Shafer theory, or a linear belief function in particular, a linear regression model may be represented as a partially swept matrix, which can be combined with similar matrices representing observations and other assumed normal distributions and state equations. The combination of swept or unswept matrices provides an alternative method for estimating linear regression models.Estimation methods[edit]\nA large number of procedures have been developed for parameter estimation and inference in linear regression. These methods differ in computational simplicity of algorithms, presence of a closed-form solution, robustness with respect to heavy-tailed distributions, and theoretical assumptions needed to validate desirable statistical properties such as consistency and asymptotic efficiency.\nSome of the more common estimation techniques for linear regression are summarized below.\n\nLeast-squares estimation and related techniques[edit]\n  Francis Galton's 1875 illustration of the correlation between the heights of adults and their parents. The observation that adult children's heights tended to deviate less from the mean height than their parents suggested the concept of \"regression toward the mean\", giving regression its name. The \"locus of horizontal tangential points\" passing through the leftmost and rightmost points on the ellipse (which is a level curve of the bivariate normal distribution estimated from the data) is the OLS estimate of the regression of parents' heights on children's heights, while the \"locus of vertical tangential points\" is the OLS estimate of the regression of children's heights on parent's heights. The major axis of the ellipse is the TLS estimate.\nMain article: Linear least squares\nLinear least squares methods include mainly:\n\nOrdinary least squares\nWeighted least squares\nGeneralized least squaresMaximum-likelihood estimation and related techniques[edit]\nMaximum likelihood estimation can be performed when the distribution of the error terms is known to belong to a certain parametric family ƒθ of probability distributions.[12] When fθ is a normal distribution with zero mean and variance θ, the resulting estimate is identical to the OLS estimate. GLS estimates are maximum likelihood estimates when ε follows a multivariate normal distribution with a known covariance matrix.\nRidge regression[13][14][15] and other forms of penalized estimation, such as Lasso regression,[5] deliberately introduce bias into the estimation of β in order to reduce the variability of the estimate. The resulting estimates generally have lower mean squared error than the OLS estimates, particularly when multicollinearity is present or when overfitting is a problem. They are generally used when the goal is to predict the value of the response variable y for values of the predictors x that have not yet been observed. These methods are not as commonly used when the goal is inference, since it is difficult to account for the bias.\nLeast absolute deviation (LAD) regression is a robust estimation technique in that it is less sensitive to the presence of outliers than OLS (but is less efficient than OLS when no outliers are present). It is equivalent to maximum likelihood estimation under a Laplace distribution model for ε.[16]\nAdaptive estimation. If we assume that error terms are independent of the regressors, εi⊥xi{\\displaystyle \\varepsilon _{i}\\perp \\mathbf {x} _{i}}, then the optimal estimator is the 2-step MLE, where the first step is used to non-parametrically estimate the distribution of the error term.[17]Other estimation techniques[edit]\n  Comparison of the Theil–Sen estimator (black) and simple linear regression (blue) for a set of points with outliers.\nBayesian linear regression applies the framework of Bayesian statistics to linear regression. (See also Bayesian multivariate linear regression.) In particular, the regression coefficients β are assumed to be random variables with a specified prior distribution.  The prior distribution can bias the solutions for the regression coefficients, in a way similar to (but more general than) ridge regression or lasso regression.  In addition, the Bayesian estimation process produces not a single point estimate for the \"best\" values of the regression coefficients but an entire posterior distribution, completely describing the uncertainty surrounding the quantity.  This can be used to estimate the \"best\" coefficients using the mean, mode, median, any quantile (see quantile regression), or any other function of the posterior distribution.\nQuantile regression focuses on the conditional quantiles of y given X rather than the conditional mean of y given X. Linear quantile regression models a particular conditional quantile, for example the conditional median, as a linear function βTx of the predictors.\nMixed models are widely used to analyze linear regression relationships involving dependent data when the dependencies have a known structure. Common applications of mixed models include analysis of data involving repeated measurements, such as longitudinal data, or data obtained from cluster sampling. They are generally fit as parametric models, using maximum likelihood or Bayesian estimation. In the case where the errors are modeled as normal random variables, there is a close connection between mixed models and generalized least squares.[18]Fixed effects estimation is an alternative approach to analyzing this type of data.\nPrincipal component regression (PCR)[7][8] is used when the number of predictor variables is large, or when strong correlations exist among the predictor variables. This two-stage procedure first reduces the predictor variables using principal component analysis then uses the reduced variables in an OLS regression fit. While it often works well in practice, there is no general theoretical reason that the most informative linear function of the predictor variables should lie among the dominant principal components of the multivariate distribution of the predictor variables. The partial least squares regression is the extension of the PCR method which does not suffer from the mentioned deficiency.\nLeast-angle regression[6] is an estimation procedure for linear regression models that was developed to handle high-dimensional covariate vectors, potentially with more covariates than observations.\nThe Theil–Sen estimator is a simple robust estimation technique that chooses the slope of the fit line to be the median of the slopes of the lines through pairs of sample points. It has similar statistical efficiency properties to simple linear regression but is much less sensitive to outliers.[19]\nOther robust estimation techniques, including the α-trimmed mean approach[citation needed], and L-, M-, S-, and R-estimators have been introduced.[citation needed]Applications[edit]\nSee also: Linear least squares § Applications\nLinear regression is widely used in biological, behavioral and social sciences to describe possible relationships between variables. It ranks as one of the most important tools used in these disciplines.\n\nTrend line[edit]\nMain article: Trend estimation\nA trend line represents a trend, the long-term movement in time series data after other components have been accounted for. It tells whether a particular data set (say GDP, oil prices or stock prices) have increased or decreased over the period of time. A trend line could simply be drawn by eye through a set of data points, but more properly their position and slope is calculated using statistical techniques like linear regression. Trend lines typically are straight lines, although some variations use higher degree polynomials depending on the degree of curvature desired in the line.\nTrend lines are sometimes used in business analytics to show changes in data over time. This has the advantage of being simple. Trend lines are often used to argue that a particular action or event (such as training, or an advertising campaign) caused observed changes at a point in time. This is a simple technique, and does not require a control group, experimental design, or a sophisticated analysis technique. However, it suffers from a lack of scientific validity in cases where other potential changes can affect the data.\n\nEpidemiology[edit]\nEarly evidence relating tobacco smoking to mortality and morbidity came from observational studies employing regression analysis. In order to reduce spurious correlations when analyzing observational data, researchers usually include several variables in their regression models in addition to the variable of primary interest. For example, in a regression model in which cigarette smoking is the independent variable of primary interest and the dependent variable is lifespan measured in years, researchers might include education and income as additional independent variables, to ensure that any observed effect of smoking on lifespan is not due to those other socio-economic factors. However, it is never possible to include all possible confounding variables in an empirical analysis. For example, a hypothetical gene might increase mortality and also cause people to smoke more. For this reason, randomized controlled trials are often able to generate more compelling evidence of causal relationships than can be obtained using regression analyses of observational data. When controlled experiments are not feasible, variants of regression analysis such as instrumental variables regression may be used to attempt to estimate causal relationships from observational data.\n\nFinance[edit]\nThe capital asset pricing model uses linear regression as well as the concept of beta for analyzing and quantifying the systematic risk of an investment. This comes directly from the beta coefficient of the linear regression model that relates the return on the investment to the return on all risky assets.\n\nEconomics[edit]\nMain article: Econometrics\nLinear regression is the predominant empirical tool in economics.  For example, it is used to predict consumption spending,[20]fixed investment spending, inventory investment, purchases of a country's exports,[21] spending on imports,[21] the demand to hold liquid assets,[22]labor demand,[23] and labor supply.[23]\nEnvironmental science[edit]\nThis section needs expansion. You can help by adding to it.  (January 2010)Linear regression finds application in a wide range of environmental science applications. In Canada, the Environmental Effects Monitoring Program uses statistical analyses on fish and benthic surveys to measure the effects of pulp mill or metal mine effluent on the aquatic ecosystem.[24]\nMachine learning[edit]\nLinear regression plays an important role in the field of artificial intelligence such as machine learning. The linear regression algorithm is one of the fundamental supervised machine-learning algorithms due to its relative simplicity and well-known properties.[25]\nHistory[edit]\nLeast squares linear regression, as a means of finding a good rough linear fit to a set of points was performed by Legendre (1805) and Gauss(1809) for the prediction of planetary movement. Quetelet was responsible for making the procedure well-known and for using it extensively in the social sciences.\n[26]\nSee also[edit]\n\nStatistics portal\n\nAnalysis of variance\nBlinder–Oaxaca decomposition\nCensored regression model\nCross-sectional regression\nCurve fitting\nEmpirical Bayes methods\nErrors and residuals\nLack-of-fit sum of squares\nLine fitting\nLinear classifier\nLinear equation\nLogistic regression\nM-estimator\nMultivariate adaptive regression splines\nNonlinear regression\nNonparametric regression\nNormal equations\nProjection pursuit regression\nSegmented linear regression\nStepwise regression\nStructural break\nSupport vector machine\nTruncated regression model\nReferences[edit]\nCitations[edit]\n\n^ David A. Freedman (2009). Statistical Models: Theory and Practice. Cambridge University Press. p. 26. A simple regression equation has on the right hand side an intercept and an explanatory variable with a slope coefficient. A multiple regression equation has two or more explanatory variables on the right hand side, each with its own slope coefficient.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:\"\\\"\"\"\\\"\"\"'\"\"'\"}.mw-parser-output .citation .cs1-lock-free a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}\n\n^ Rencher, Alvin C.; Christensen, William F. (2012), \"Chapter 10, Multivariate regression – Section 10.1, Introduction\", Methods of Multivariate Analysis, Wiley Series in Probability and Statistics, 709 (3rd ed.), John Wiley & Sons, p. 19, ISBN 9781118391679.\n\n^ Hilary L. Seal (1967). \"The historical development of the Gauss linear model\". Biometrika. 54 (1/2): 1–24. doi:10.1093/biomet/54.1-2.1. JSTOR 2333849.\n\n^ Yan, Xin (2009), Linear Regression Analysis: Theory and Computing, World Scientific, pp. 1–2, ISBN 9789812834119, Regression analysis ... is probably one of the oldest topics in mathematical statistics dating back to about two hundred years ago. The earliest form of the linear regression was the least squares method, which was published by Legendre in 1805, and by Gauss in 1809 ... Legendre and Gauss both applied the method to the problem of determining, from astronomical observations, the orbits of bodies about the sun.\n\n^ a b Tibshirani, Robert (1996). \"Regression Shrinkage and Selection via the Lasso\". Journal of the Royal Statistical Society, Series B. 58 (1): 267–288. JSTOR 2346178.\n\n^ a b Efron, Bradley; Hastie, Trevor; Johnstone, Iain; Tibshirani, Robert (2004). \"Least Angle Regression\". The Annals of Statistics. 32 (2): 407–451. arXiv:math/0406456. doi:10.1214/009053604000000067. JSTOR 3448465.\n\n^ a b Hawkins, Douglas M. (1973). \"On the Investigation of Alternative Regressions by Principal Component Analysis\". Journal of the Royal Statistical Society, Series C. 22 (3): 275–286. JSTOR 2346776.\n\n^ a b Jolliffe, Ian T. (1982). \"A Note on the Use of Principal Components in Regression\". Journal of the Royal Statistical Society, Series C. 31 (3): 300–303. JSTOR 2348005.\n\n^ Berk, Richard A. (2007). \"Regression Analysis: A Constructive Critique\". Criminal Justice Review. 32 (3): 301–302. doi:10.1177/0734016807304871.\n\n^ Warne, Russell T. (2011). \"Beyond multiple regression: Using commonality analysis to better understand R2 results\". Gifted Child Quarterly. 55 (4): 313–318. doi:10.1177/0016986211422217.\n\n^ Brillinger, David R. (1977). \"The Identification of a Particular Nonlinear Time Series System\". Biometrika. 64 (3): 509–515. doi:10.1093/biomet/64.3.509. JSTOR 2345326.\n\n^ Lange, Kenneth L.; Little, Roderick J. A.; Taylor, Jeremy M. G. (1989). \"Robust Statistical Modeling Using the t Distribution\". Journal of the American Statistical Association. 84 (408): 881–896. doi:10.2307/2290063. JSTOR 2290063.\n\n^ Swindel, Benee F. (1981). \"Geometry of Ridge Regression Illustrated\". The American Statistician. 35 (1): 12–15. doi:10.2307/2683577. JSTOR 2683577.\n\n^ Draper, Norman R.; van Nostrand; R. Craig (1979). \"Ridge Regression and James-Stein Estimation: Review and Comments\". Technometrics. 21 (4): 451–466. doi:10.2307/1268284. JSTOR 1268284.\n\n^ Hoerl, Arthur E.; Kennard, Robert W.; Hoerl, Roger W. (1985). \"Practical Use of Ridge Regression: A Challenge Met\". Journal of the Royal Statistical Society, Series C. 34 (2): 114–120. JSTOR 2347363.\n\n^ Narula, Subhash C.; Wellington, John F. (1982). \"The Minimum Sum of Absolute Errors Regression: A State of the Art Survey\". International Statistical Review. 50 (3): 317–326. doi:10.2307/1402501. JSTOR 1402501.\n\n^ Stone, C. J. (1975). \"Adaptive maximum likelihood estimators of a location parameter\". The Annals of Statistics. 3 (2): 267–284. doi:10.1214/aos/1176343056. JSTOR 2958945.\n\n^ Goldstein, H. (1986). \"Multilevel Mixed Linear Model Analysis Using Iterative Generalized Least Squares\". Biometrika. 73 (1): 43–56. doi:10.1093/biomet/73.1.43. JSTOR 2336270.\n\n^ Theil, H. (1950). \"A rank-invariant method of linear and polynomial regression analysis. I, II, III\". Nederl. Akad. Wetensch., Proc. 53: 386–392, 521–525, 1397–1412. MR 0036489; Sen, Pranab Kumar (1968). \"Estimates of the regression coefficient based on Kendall's tau\". Journal of the American Statistical Association. 63 (324): 1379–1389. doi:10.2307/2285891. JSTOR 2285891. MR 0258201.\n\n^ Deaton, Angus (1992). Understanding Consumption. Oxford University Press. ISBN 978-0-19-828824-4.\n\n^ a b Krugman, Paul R.; Obstfeld, M.; Melitz, Marc J. (2012). International Economics: Theory and Policy (9th global ed.). Harlow: Pearson. ISBN 9780273754091.\n\n^ Laidler, David E. W. (1993). The Demand for Money: Theories, Evidence, and Problems (4th ed.). New York: Harper Collins. ISBN 978-0065010985.\n\n^ a b Ehrenberg; Smith (2008). Modern Labor Economics (10th international ed.). London: Addison-Wesley. ISBN 9780321538963.\n\n^ EEMP webpage Archived 2011-06-11 at the Wayback Machine\n\n^ \"Linear Regression (Machine Learning)\" (PDF). University of Pittsburgh.\n\n^ \nStigler, Stephen M. (1986). The History of Statistics: The Measurement of Uncertainty before 1900. Cambridge: Harvard. ISBN 0-674-40340-1.\n\n\nSources[edit]\n.mw-parser-output .refbegin{font-size:90%;margin-bottom:0.5em}.mw-parser-output .refbegin-hanging-indents>ul{list-style-type:none;margin-left:0}.mw-parser-output .refbegin-hanging-indents>ul>li,.mw-parser-output .refbegin-hanging-indents>dl>dd{margin-left:0;padding-left:3.2em;text-indent:-3.2em;list-style:none}.mw-parser-output .refbegin-100{font-size:100%}\nCohen, J., Cohen P., West, S.G., & Aiken, L.S. (2003). Applied multiple regression/correlation analysis for the behavioral sciences. (2nd ed.) Hillsdale, NJ: Lawrence Erlbaum Associates\nCharles Darwin. The Variation of Animals and Plants under Domestication. (1868) (Chapter XIII describes what was known about reversion in Galton's time. Darwin uses the term \"reversion\".)\nDraper, N.R.; Smith, H. (1998). Applied Regression Analysis (3rd ed.). John Wiley. ISBN 978-0-471-17082-2.\nFrancis Galton. \"Regression Towards Mediocrity in Hereditary Stature,\" Journal of the Anthropological Institute, 15:246-263 (1886). (Facsimile at: [1])\nRobert S. Pindyck and Daniel L. Rubinfeld (1998, 4h ed.). Econometric Models and Economic Forecasts, ch. 1 (Intro, incl. appendices on Σ operators & derivation of parameter est.) & Appendix 4.3 (mult. regression in matrix form).\nFurther reading[edit]\nPedhazur, Elazar J (1982). Multiple regression in behavioral research: Explanation and prediction (2nd ed.). New York: Holt, Rinehart and Winston. ISBN 978-0-03-041760-3.\nMathieu Rouaud, 2013: Probability, Statistics and Estimation Chapter 2: Linear Regression, Linear Regression with Error Bars and Nonlinear Regression.\nNational Physical Laboratory (1961). \"Chapter 1: Linear Equations and Matrices: Direct Methods\". Modern Computing Methods. Notes on Applied Science. 16 (2nd ed.). Her Majesty's Stationery OfficeExternal links[edit]\n\nWikiversity has learning resources about Linear regression\nThe Wikibook R Programming has a page on the topic of: Linear Models\nWikimedia Commons has media related to Linear regression.Least-Squares Regression, PhET Interactive simulations, University of Colorado at Boulder\nDIY Linear Fit\nvteLeast squares and regression analysisComputational statistics\nLeast squares\nLinear least squares\nNon-linear least squares\nIteratively reweighted least squaresCorrelation and dependence\nPearson product-moment correlation\nRank correlation (Spearman's rho\nKendall's tau)\nPartial correlation\nConfounding variableRegression analysis\nOrdinary least squares\nPartial least squares\nTotal least squares\nRidge regressionRegression as a statistical modelLinear regression\nSimple linear regression\nOrdinary least squares\nGeneralized least squares\nWeighted least squares\nGeneral linear modelPredictor structure\nPolynomial regression\nGrowth curve (statistics)\nSegmented regression\nLocal regressionNon-standard\nNonlinear regression\nNonparametric\nSemiparametric\nRobust\nQuantile\nIsotonicNon-normal errors\nGeneralized linear model\nBinomial\nPoisson\nLogisticDecomposition of variance\nAnalysis of variance\nAnalysis of covariance\nMultivariate AOVModel exploration\nStepwise regression\nModel selection\nMallows's Cp\nAIC\nBIC\nModel specification\nRegression validationBackground\nMean and predicted response\nGauss–Markov theorem\nErrors and residuals\nGoodness of fit\nStudentized residual\nMinimum mean-square errorDesign of experiments\nResponse surface methodology\nOptimal design\nBayesian designNumerical approximation\nNumerical analysis\nApproximation theory\nNumerical integration\nGaussian quadrature\nOrthogonal polynomials\nChebyshev polynomials\nChebyshev nodesApplications\nCurve fitting\nCalibration curve\nNumerical smoothing and differentiation\nSystem identification\nMoving least squares\nRegression analysis category\nStatistics category\nStatistics portal\nStatistics outline\nStatistics topics\nvteStatistics\nOutline\nIndexDescriptive statisticsContinuous dataCenter\nMean\narithmetic\ngeometric\nharmonic\nMedian\nModeDispersion\nVariance\nStandard deviation\nCoefficient of variation\nPercentile\nRange\nInterquartile rangeShape\nCentral limit theorem\nMoments\nSkewness\nKurtosis\nL-momentsCount data\nIndex of dispersionSummary tables\nGrouped data\nFrequency distribution\nContingency tableDependence\nPearson product-moment correlation\nRank correlation\nSpearman's rho\nKendall's tau\nPartial correlation\nScatter plotGraphics\nBar chart\nBiplot\nBox plot\nControl chart\nCorrelogram\nFan chart\nForest plot\nHistogram\nPie chart\nQ–Q plot\nRun chart\nScatter plot\nStem-and-leaf display\nRadar chartData collectionStudy design\nPopulation\nStatistic\nEffect size\nStatistical power\nOptimal design\nSample size determination\nReplication\nMissing dataSurvey methodology\nSampling\nstratified\ncluster\nStandard error\nOpinion poll\nQuestionnaireControlled experiments\nScientific control\nRandomized experiment\nRandomized controlled trial\nRandom assignment\nBlocking\nInteraction\nFactorial experimentAdaptive Designs\nAdaptive clinical trial\nUp-and-Down Designs\nStochastic approximationObservational Studies\nCross-sectional study\nCohort study\nNatural experiment\nQuasi-experimentStatistical inferenceStatistical theory\nPopulation\nStatistic\nProbability distribution\nSampling distribution\nOrder statistic\nEmpirical distribution\nDensity estimation\nStatistical model\nModel specification\nLp space\nParameter\nlocation\nscale\nshape\nParametric family\nLikelihood (monotone)\nLocation–scale family\nExponential family\nCompleteness\nSufficiency\nStatistical functional\nBootstrap\nU\nV\nOptimal decision\nloss function\nEfficiency\nStatistical distance\ndivergence\nAsymptotics\nRobustnessFrequentist inferencePoint estimation\nEstimating equations\nMaximum likelihood\nMethod of moments\nM-estimator\nMinimum distance\nUnbiased estimators\nMean-unbiased minimum-variance\nRao–Blackwellization\nLehmann–Scheffé theorem\nMedian unbiased\nPlug-inInterval estimation\nConfidence interval\nPivot\nLikelihood interval\nPrediction interval\nTolerance interval\nResampling\nBootstrap\nJackknifeTesting hypotheses\n1- & 2-tails\nPower\nUniformly most powerful test\nPermutation test\nRandomization test\nMultiple comparisonsParametric tests\nLikelihood-ratio\nLagrange multiplier\nWaldSpecific tests\nZ-test (normal)\nStudent's t-test\nF-testGoodness of fit\nChi-squared\nG-test\nKolmogorov–Smirnov\nAnderson–Darling\nLilliefors\nJarque–Bera\nNormality (Shapiro–Wilk)\nLikelihood-ratio test\nModel selection\nCross validation\nAIC\nBICRank statistics\nSign\nSample median\nSigned rank (Wilcoxon)\nHodges–Lehmann estimator\nRank sum (Mann–Whitney)\nNonparametric anova\n1-way (Kruskal–Wallis)\n2-way (Friedman)\nOrdered alternative (Jonckheere–Terpstra)Bayesian inference\nBayesian probability\nprior\nposterior\nCredible interval\nBayes factor\nBayesian estimator\nMaximum posterior estimatorCorrelationRegression analysisCorrelation\nPearson product-moment\nPartial correlation\nConfounding variable\nCoefficient of determinationRegression analysis\nErrors and residuals\nRegression validation\nMixed effects models\nSimultaneous equations models\nMultivariate adaptive regression splines (MARS)Linear regression\nSimple linear regression\nOrdinary least squares\nGeneral linear model\nBayesian regressionNon-standard predictors\nNonlinear regression\nNonparametric\nSemiparametric\nIsotonic\nRobust\nHeteroscedasticity\nHomoscedasticityGeneralized linear model\nExponential families\nLogistic (Bernoulli) / Binomial / Poisson regressionsPartition of variance\nAnalysis of variance (ANOVA, anova)\nAnalysis of covariance\nMultivariate ANOVA\nDegrees of freedomCategorical / Multivariate / Time-series / Survival analysisCategorical\nCohen's kappa\nContingency table\nGraphical model\nLog-linear model\nMcNemar's testMultivariate\nRegression\nManova\nPrincipal components\nCanonical correlation\nDiscriminant analysis\nCluster analysis\nClassification\nStructural equation model\nFactor analysis\nMultivariate distributions\nElliptical distributions\nNormalTime-seriesGeneral\nDecomposition\nTrend\nStationarity\nSeasonal adjustment\nExponential smoothing\nCointegration\nStructural break\nGranger causalitySpecific tests\nDickey–Fuller\nJohansen\nQ-statistic (Ljung–Box)\nDurbin–Watson\nBreusch–GodfreyTime domain\nAutocorrelation (ACF)\npartial (PACF)\nCross-correlation (XCF)\nARMA model\nARIMA model (Box–Jenkins)\nAutoregressive conditional heteroskedasticity (ARCH)\nVector autoregression (VAR)Frequency domain\nSpectral density estimation\nFourier analysis\nWavelet\nWhittle likelihoodSurvivalSurvival function\nKaplan–Meier estimator (product limit)\nProportional hazards models\nAccelerated failure time (AFT) model\nFirst hitting timeHazard function\nNelson–Aalen estimatorTest\nLog-rank testApplicationsBiostatistics\nBioinformatics\nClinical trials / studies\nEpidemiology\nMedical statisticsEngineering statistics\nChemometrics\nMethods engineering\nProbabilistic design\nProcess / quality control\nReliability\nSystem identificationSocial statistics\nActuarial science\nCensus\nCrime statistics\nDemography\nEconometrics\nNational accounts\nOfficial statistics\nPopulation statistics\nPsychometricsSpatial statistics\nCartography\nEnvironmental statistics\nGeographic information system\nGeostatistics\nKriging\nCategory\nPortal\nCommons\nWikiProject\n\n\n\n\n\n\t\t\n\t\tRetrieved from \"https://en.wikipedia.org/w/index.php?title=Linear_regression&oldid=889485632\"\n\t\t\n\t\tCategories: Single-equation methods (econometrics)Estimation theoryParametric statisticsHidden categories: Articles with inconsistent citation formatsWebarchive template wayback linksWikipedia articles needing clarification from May 2018Wikipedia articles needing clarification from March 2012All articles with unsourced statementsArticles with unsourced statements from June 2018Articles to be expanded from January 2010All articles to be expandedArticles using small message boxes\n\t\t\n\t\t\n\t\t\n\t\n\n\n\t\t\n\t\t\tNavigation menu\n\t\t\t\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\tPersonal tools\n\t\t\t\t\t\tNot logged inTalkContributionsCreate accountLog in\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\tNamespaces\n\t\t\t\t\t\tArticleTalk\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tVariants\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\tViews\n\t\t\t\t\t\tReadEditView history\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\tMore\n\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\tSearch\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\t\t\t\n\t\t\tNavigation\n\t\t\t\n\t\t\t\t\t\t\t\tMain pageContentsFeatured contentCurrent eventsRandom articleDonate to WikipediaWikipedia store\t\t\t\t\n\t\t\n\t\t\t\n\t\t\tInteraction\n\t\t\t\n\t\t\t\t\t\t\t\tHelpAbout WikipediaCommunity portalRecent changesContact page\t\t\t\t\n\t\t\n\t\t\t\n\t\t\tTools\n\t\t\t\n\t\t\t\t\t\t\t\tWhat links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationWikidata itemCite this page\t\t\t\t\n\t\t\n\t\t\t\n\t\t\tPrint/export\n\t\t\t\n\t\t\t\t\t\t\t\tCreate a bookDownload as PDFPrintable version\t\t\t\t\n\t\t\n\t\t\t\n\t\t\tIn other projects\n\t\t\t\n\t\t\t\t\t\t\t\tWikimedia Commons\t\t\t\t\n\t\t\n\t\t\t\n\t\t\tLanguages\n\t\t\t\n\t\t\t\t\t\t\t\tالعربيةAsturianuCatalàČeštinaDeutschEestiΕλληνικάEspañolEuskaraفارسیFrançaisGalego한국어HrvatskiBahasa IndonesiaItalianoעבריתMagyarМакедонски日本語NorskPolskiPortuguêsРусскийSimple EnglishSlovenščinaСрпски / srpskiSvenskaУкраїнськаTiếng Việt中文\t\t\t\tEdit links\t\t\t\n\t\t\n\t\t\t\t\n\t\t\n\t\t\t\t\n\t\t\t\t\t\t This page was last edited on 26 March 2019, at 00:18 (UTC).\n\t\t\t\t\t\t\t\tText is available under the Creative Commons Attribution-ShareAlike License;\nadditional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\n\t\t\t\t\t\t\tPrivacy policy\n\t\t\t\t\t\t\t\tAbout Wikipedia\n\t\t\t\t\t\t\t\tDisclaimers\n\t\t\t\t\t\t\t\tContact Wikipedia\n\t\t\t\t\t\t\t\tDevelopers\n\t\t\t\t\t\t\t\tCookie statement\n\t\t\t\t\t\t\t\tMobile view\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\n\t\t\n\t\t\n\n(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgPageParseReport\":{\"limitreport\":{\"cputime\":\"0.712\",\"walltime\":\"1.036\",\"ppvisitednodes\":{\"value\":3716,\"limit\":1000000},\"ppgeneratednodes\":{\"value\":0,\"limit\":1500000},\"postexpandincludesize\":{\"value\":260789,\"limit\":2097152},\"templateargumentsize\":{\"value\":5879,\"limit\":2097152},\"expansiondepth\":{\"value\":12,\"limit\":40},\"expensivefunctioncount\":{\"value\":9,\"limit\":500},\"unstrip-depth\":{\"value\":1,\"limit\":20},\"unstrip-size\":{\"value\":88103,\"limit\":5000000},\"entityaccesscount\":{\"value\":4,\"limit\":400},\"timingprofile\":[\"100.00%  621.431      1 -total\",\" 44.14%  274.305      1 Template:Reflist\",\" 21.91%  136.148     17 Template:Cite_journal\",\" 14.45%   89.802      9 Template:Cite_book\",\" 10.54%   65.475      1 Template:Statistics\",\"  9.96%   61.875      2 Template:Clarify\",\"  9.75%   60.569     13 Template:Navbox\",\"  9.50%   59.033      1 Template:Navbox_with_collapsible_groups\",\"  8.93%   55.463      2 Template:Fix-span\",\"  6.20%   38.502      1 Template:Regression_bar\"]},\"scribunto\":{\"limitreport-timeusage\":{\"value\":\"0.308\",\"limit\":\"10.000\"},\"limitreport-memusage\":{\"value\":7129754,\"limit\":52428800},\"limitreport-logs\":\"table#1 {\\n  [\\\"size\\\"] = \\\"tiny\\\",\\n}\\n\"},\"cachereport\":{\"origin\":\"mw1320\",\"timestamp\":\"20190328023619\",\"ttl\":2592000,\"transientcontent\":false}}});mw.config.set({\"wgBackendResponseTime\":1223,\"wgHostname\":\"mw1320\"});});"
```

```r
html_attrs(MLR_html) # Extract attributes
```

```
##         class          lang           dir 
## "client-nojs"          "en"         "ltr"
```

```r
html_table(html_nodes(MLR_html, "table")[[1]]) # Parse an html table into a data frame.
```

```
##                                                                                                                                                                    X1
## 1                                                                                                                                      Part of a series on Statistics
## 2                                                                                                                                                 Regression analysis
## 3                                                                                                                                                                    
## 4                                                                                                                                                              Models
## 5                                                                                   Linear regression\nSimple regression\nPolynomial regression\nGeneral linear model
## 6  Generalized linear model\nDiscrete choice\nLogistic regression\nMultinomial logit\nMixed logit\nProbit\nMultinomial probit\nOrdered logit\nOrdered probit\nPoisson
## 7                                                                                                        Multilevel model\nFixed effects\nRandom effects\nMixed model
## 8                                Nonlinear regression\nNonparametric\nSemiparametric\nRobust\nQuantile\nIsotonic\nPrincipal components\nLeast angle\nLocal\nSegmented
## 9                                                                                                                                                 Errors-in-variables
## 10                                                                                                                                                         Estimation
## 11                                                                                                                                  Least squares\nLinear\nNon-linear
## 12                                                                                                                                    Ordinary\nWeighted\nGeneralized
## 13                                                                                                        Partial\nTotal\nNon-negative\nRidge regression\nRegularized
## 14                                                                                 Least absolute deviations\nIteratively reweighted\nBayesian\nBayesian multivariate
## 15                                                                                                                                                         Background
## 16                              Regression validation\nMean and predicted response\nErrors and residuals\nGoodness of fit\nStudentized residual\nGauss–Markov theorem
## 17                                                                                                                                                  Statistics portal
## 18                                                                                                                                                                vte
```

```r
MLR_html %>% html_nodes("table") %>% `[`(1:2) %>% html_nodes("img")  # Extract all images in the first two tables
```

```
## {xml_nodeset (3)}
## [1] <img alt="Linear regression.svg" src="//upload.wikimedia.org/wikiped ...
## [2] <img alt="Fisher iris versicolor sepalwidth.svg" src="//upload.wikim ...
## [3] <img alt="[icon]" src="//upload.wikimedia.org/wikipedia/commons/thum ...
```

---

