---
title: "Blog 5 - reproducibility contd."
author: "Lydia English"
date: "February 18, 2019"
layout: post
root: ../../../
---

1. **Answer each of the following questions with at least 2-3 sentences.**

    1. **Summarize Roger Peng's outline for reproducibility in** *Biostatistics*.
    
      Peng's outline for reproducibility in the journal *Biostatistics* entails authors sending data and code that can be executed in order to achieve the same results described in their paper. It is the responsibility of one person at the journal, the Associate Editor for reproducibility, to ensure that the results in the paper match those that can be reproduced using said data and code. 
    
    2. **What are Keiding's main criticisms of the proposal?**. 
    
      Keidings main criticisms of this reproducibility outline are that it ignores all the context and decisions leading up to the creation of the "master data set." Keiding argues that reproducibility is not just making sure your computer can punch out the same numbers as someone elses, but that it entails an evaluation of the substantive context underyling a dataset. In other words, what are the decisions made by the scientists and statistians in order to answer the research questions. Without knowledge of this prior context and decision justification, reproducing results using code and data is incomplete. 
    
    3. **Which point in the commentaries speaks to you the most? Why?**
    
    I actually found Peng's response to Keidings opinion to be the most engaging of all the commentaries. Peng admits that having just that dataset and the code is no way to completely reproduce someone's findings but that it is the recipe that leads to their conclusions. And while it may seem "minimal" to give just the code for the analysis, in certain cases it can be quite difficult for people to acheive even that. 
    
    4. **How does Keiding respond to the commentary you picked. What are his main points in his response?**
    
    Keiding respond's to Peng's commentary by acknowleding that these reproducibility standards as originally outlined are indeed minimal and that, even with improved documentation, it may not be enough. He seems to settle on the conclusion that improved transparency is good, but will not be enough to fully capture and reproduce the scientific process. 
    
    Overall I find Keiding's argument, while valid, to be unproductive and pedantic. Of COURSE we need every single piece of information to fully reproduce something, but it's very unlikely we will get to that place. Having some data and analytical code to work with is a great baseline. 
    
2. **Describe your experience with a data intensive (collaborative) project. What are the most prominent issues with ensuring reproducibility of research results (focus on data related aspects)? What would you do differently if you could go back in time?**

  Right now I work on a large, interdisciplinary project that has been around for more than 10 years. As the project has increased in size and complexity the team is facing new challenges in integrating data into consistent formats and analyses. I recently sat-in on a meeting where some people were looking at old data and trying to figure out what they were seeing and how they could analyze it to get the same results. The problem is not that documents didn't exist detailing this, but that they were hidden deep in other folders and the original person performing the analysis had left Iowa State years ago. 
  
  It's hard at the beginning of projects to have the foresight to develop data management protocols but if I could go back in time I would highly encourage the project to recruit a person to design data management standards and protocols from the start. That way things would be consistent across all areas of the project, and the data wouldn't be so dependent on the person conducting the experiment. 
