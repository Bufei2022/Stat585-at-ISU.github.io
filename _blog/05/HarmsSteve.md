---
title: "Reproducibility part two"
author: "Steve Harms"
topic: "05"
layout: post
root: ../../../
---

1. **Answer each of the following questions with at least 2-3 sentences.**

    1. **Summarize Roger Peng's outline for reproducibility in** *Biostatistics*. 
    The outline for reproducibility has three simple dimensions: Data, code, and reproducible. It simply requires authors to publish all of their data if possible (given constraints due to confidentiality, etc.) in a standard format, as well as all of their code and software packages (i.e., R) in an organized and readable manner. Sources and references should be clearly stated. The third dimension is a result of the other two, in that reviewers must be able to use the code and data to reproduce the results (up to some degree of tolerance). It should be noted that Peng provides many caveats to the above situations as well.
    
    2. **What are Keiding's main criticisms of the proposal?**. 
    Keiding's main criticisms revolve around the idea that simply on reproducing exact numerical results causes the statistician to divorce the analysis from its wider scientific context. We are trying only to make sure our p-values and confidence intervals match, but do we draw the same conclusions about the data for those inferential results? What if we took these results to a different field of experts? This is not addressed by Peng's article. Moreover, the focus on constant reanalysis and brow-beating non-statisticians for minor errors that do not change the actual results and/or ignore the scientific problem that motivated the analysis makes our field look bad.
    Another criticism is that statisticians often use oversimplified datasets to investigate their new methods, instead of being motivated by an actual scientific problem. This means that we're not really producing useful results, even if they're reproducible. I think this is a valid point, but also somewhat misses the point of Peng's original article, which was just providing a standard for submissions rather than a methodological/philosophical debate about why we conduct certain analyses.
    
    3. **Which point in the commentaries speaks to you the most? Why?**
    The "4 good reasons" to bother ourselves with reproducibility from Goodman's commentary struck me the most. He focuses on the crossover from industry to academia, and how the inclusion of statistical reproducbility standards are important. The best point he made regarded the need for actual statisticians to consult on analysis and re-analysis in order to ensure scientific integrity. He makes the point that, even if the original author(s) have code and data that are technically correct, there is are more subtle issues of the experimental design, data collection, and (most importantly) choice and interpretation of methods that a non-statistician reviewer may miss. This is important because of the wide range of approaches to analysis that might produce differing results and conclusions for the same data. As statisticians, we have enough wars over the interpretation of p-values, which might be one of the most basic of analytical results we obtain. Are we really willing to let all of the other possible methodological/interpretation errors go as well, if only we can type in the code and get the same numbers?
    
    4. **How does Keiding respond to the commentary you picked. What are his main points in his response?**
    Keiding does not directly respond to the point made above, largely because it is very similar to the point he originally made. He does commend Goodman's suggestion that providing full documentation of the transformational process from "real-world" to "basic dataset" will aid in statistical referreeing of articles. In a sense, Goodman was stating more concretely Keiding's original statement that too often we focus on basic computation and mechanics of reproducibility rather than considering the larger context or the plethora of alternative methods in the design of the analysis.
    
2. **Describe your experience with a data intensive (collaborative) project. What are the most prominent issues with ensuring reproducibility of research results (focus on data related aspects)? What would you do differently if you could go back in time?**

I haven't had experience in academia with too many collaborative projects, but I have had some in my work experience. The most prominent issue in my experience is not the sharing of code/data, but in the analytical methods. I have found it very easy to try many different models until I find one that produces a "pretty" or "useful" result for my dataset, and then publishing only that one model (which can then be reporduced). I think this happens much more often than we like to believe, both in industry (we need results!) and in academic statistic publications (we want to show how our new method is useful!), and it's hard to tell when someone else does it because we only need to adhere to the "minimal" standard of publishing the code that produced the results we want to publish. The solution to this is to require more refereeing by statisticians. Instead of just running the R code, instead give only the data and (maybe) a scientific question and let the independent statistician conduct their own analysis with their method of choice. Are the results the same? Have we been transparent in our reasoning for a choice of methods? If not, we should consider it.
